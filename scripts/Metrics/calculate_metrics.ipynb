{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:57:55.447521Z",
     "start_time": "2024-11-27T12:57:55.438522Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import scipy.spatial.distance"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T13:12:55.037636Z",
     "start_time": "2024-11-27T13:12:55.024636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Change directory to the root of the SpatialDeconBenchmark repository\n",
    "os.chdir('/'.join(re.match(r'^(.*SpatialDeconBenchmark)', os.getcwd()).group(0).split('/')))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to: C:\\Users\\Lem\\PycharmProjects\\SpatialDeconBenchmark\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:57:55.494585Z",
     "start_time": "2024-11-27T12:57:55.470521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Metrics:\n",
    "    def __init__(self, method_name, ground_truth_mtx, deconvolved_mtx):\n",
    "        \"\"\"\n",
    "        Initialize the Metrics object with method, ground truth matrix, and deconvolved matrix.\n",
    "        \"\"\"\n",
    "        self.method = method_name\n",
    "        self.ground_truth_mtx = ground_truth_mtx\n",
    "        self.deconvolved_mtx = deconvolved_mtx\n",
    "        self.num_cell_types = None\n",
    "        self.num_spots = None\n",
    "        self.rmse = None\n",
    "        self.jsd = None\n",
    "\n",
    "    def check_mtx_format(self):\n",
    "        \"\"\"\n",
    "        Ensures matrices are in the correct format (grid by cell type) with matching indices and columns.\n",
    "        \"\"\"\n",
    "        # Check if cell type labels match\n",
    "        if set(self.ground_truth_mtx.columns) != set(self.deconvolved_mtx.columns):\n",
    "            raise ValueError(\"Cell type labels do not match\")\n",
    "\n",
    "        # Check if grid labels match\n",
    "        if set(self.ground_truth_mtx.index) != set(self.deconvolved_mtx.index):\n",
    "            raise ValueError(\"Spot labels do not match\")\n",
    "        \n",
    "        self.deconvolved_mtx = self.deconvolved_mtx.loc[self.ground_truth_mtx.index, self.ground_truth_mtx.columns]\n",
    "        self.num_cell_types = self.deconvolved_mtx.shape[1]\n",
    "        self.num_spots = self.deconvolved_mtx.shape[0]\n",
    "\n",
    "    def calculate_rmse(self):\n",
    "        \"\"\"\n",
    "        Calculates RMSE from the ground truth and deconvolved matrix.\n",
    "        Returns a dictionary with average RMSE and RMSE per cell type.\n",
    "        \"\"\"\n",
    "        sum_mse = 0\n",
    "        rmse_per_cell_type = []\n",
    "        \n",
    "        for i in range(self.num_cell_types):\n",
    "            p_true = self.ground_truth_mtx.iloc[:, i]\n",
    "            p_pred = self.deconvolved_mtx.iloc[:, i]\n",
    "        \n",
    "            mse = np.sum((p_true - p_pred)**2)\n",
    "            sum_mse += mse\n",
    "            rmse_per_cell_type.append(np.sqrt(mse / self.num_spots))\n",
    "            \n",
    "        rmse_per_cell_type = pd.Series(rmse_per_cell_type, index=self.ground_truth_mtx.columns)\n",
    "\n",
    "        avg_rmse = np.sqrt(sum_mse / (self.num_spots * self.num_cell_types)) \n",
    "\n",
    "        self.rmse = {\"Average RMSE\": avg_rmse, \"RMSE per cell type\": rmse_per_cell_type}\n",
    "\n",
    "        return self.rmse\n",
    "    \n",
    "    def calculate_jsd(self, use_scipy=True):\n",
    "        \"\"\"\n",
    "        Compute Jensen-Shannon divergence per cell type using either the scipy implementation or a custom method.\n",
    "        Returns JSD per cell type.\n",
    "        \"\"\"\n",
    "        jsd_per_cell_type = []\n",
    "\n",
    "        for i in range(self.num_cell_types):\n",
    "            p_true = self.ground_truth_mtx.iloc[:, i]\n",
    "            p_pred = self.deconvolved_mtx.iloc[:, i]\n",
    "\n",
    "            if use_scipy:\n",
    "                jsd = scipy.spatial.distance.jensenshannon(p_true, p_pred)**2\n",
    "            else:\n",
    "                # Custom JSD calculation (requires verification and testing)\n",
    "                p_true_dist = scipy.stats.rv_histogram(np.histogram(p_true, bins=10))\n",
    "                p_pred_dist = scipy.stats.rv_histogram(np.histogram(p_pred, bins=10))\n",
    "\n",
    "                p_true_quantiles = [np.quantile(p_true, x) for x in [0.25, 0.5, 0.75]]\n",
    "                p_pred_quantiles = [np.quantile(p_pred, x) for x in [0.25, 0.5, 0.75]]\n",
    "                p_true_pdf = p_true_dist.pdf(p_true_quantiles)\n",
    "                p_pred_pdf = p_pred_dist.pdf(p_pred_quantiles)\n",
    "\n",
    "                mean_pdf = (p_true_pdf + p_pred_pdf) / 2\n",
    "\n",
    "                kld_true = np.sum([x * np.log(x / y) for x, y in zip(p_true_pdf, mean_pdf) if y != 0])\n",
    "                kld_pred = np.sum([x * np.log(x / y) for x, y in zip(p_pred_pdf, mean_pdf) if y != 0])\n",
    "\n",
    "                jsd = (kld_true + kld_pred) / 2\n",
    "                \n",
    "            jsd_per_cell_type.append(jsd)\n",
    "            \n",
    "        jsd_per_cell_type = pd.Series(jsd_per_cell_type, index=self.ground_truth_mtx.columns)\n",
    "        self.jsd = jsd_per_cell_type\n",
    "\n",
    "        return self.jsd"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:58:36.588434Z",
     "start_time": "2024-11-27T12:58:36.550434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ground_truth = pd.read_csv(os.getcwd() + \"/data/seqFISH/ground_truth.csv\", index_col=0)\n",
    "results_dir = os.getcwd() + \"/results/seqFISH/\"\n",
    "\n",
    "benchmark = []\n",
    "for result in os.listdir(results_dir):\n",
    "    if result.startswith(\"seqFISH\") and not 'benchmark' in result:\n",
    "        method = result.split(\"_\")[1].split(\".\")[0]\n",
    "        mtx = pd.read_csv(results_dir + result, index_col=0)\n",
    "        metrics = Metrics(method, ground_truth, mtx)\n",
    "        metrics.check_mtx_format()\n",
    "        metrics.calculate_jsd()\n",
    "        metrics.calculate_rmse()\n",
    "        \n",
    "        data = {\n",
    "        'method': metrics.method,\n",
    "        'JSD': metrics.jsd.mean(),\n",
    "        'total_RMSE': metrics.rmse['Average RMSE']\n",
    "        }\n",
    "        \n",
    "        data.update(metrics.rmse['RMSE per cell type'][1:].to_dict())\n",
    "        benchmark.append(pd.DataFrame([data]))\n",
    "        \n",
    "benchmark = pd.concat(benchmark, ignore_index=True)\n",
    "benchmark.to_csv(os.getcwd() + \"/results/seqFISH_Benchmarking_Results.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:57:55.542603Z",
     "start_time": "2024-11-27T12:57:55.527589Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
